"use strict";(self.webpackChunkuuboyscy_engineering_logs=self.webpackChunkuuboyscy_engineering_logs||[]).push([[8130],{7735:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"modern-data-engineering-milestones-key-technologies-that-shaped-the-industry","metadata":{"permalink":"/blog/modern-data-engineering-milestones-key-technologies-that-shaped-the-industry","source":"@site/blog/2024-12-19-modern-data-engineering-milestones-key-technologies-that-shaped-the-industry/index.md","title":"Modern Data Engineering Milestones: Key Technologies That Shaped the Industry","description":"","date":"2024-12-19T00:00:00.000Z","tags":[{"inline":false,"label":"dbt","permalink":"/blog/tags/dbt","description":"dbt tag description"},{"inline":false,"label":"GCP","permalink":"/blog/tags/gcp","description":"GCP tag description"},{"inline":false,"label":"AWS","permalink":"/blog/tags/aws","description":"AWS tag description"},{"inline":false,"label":"Airflow","permalink":"/blog/tags/airflow","description":"Airflow tag description"},{"inline":false,"label":"Prefect","permalink":"/blog/tags/prefect","description":"Prefect tag description"}],"readingTime":0.065,"hasTruncateMarker":false,"authors":[{"name":"uuboyscy","title":"Data Engineer | Founder of uuboyscy.dev","url":"https://uuboyscy.dev","page":{"permalink":"/blog/authors/uuboyscy"},"socials":{"github":"https://github.com/uuboyscy","linkedin":"https://www.linkedin.com/in/chengyou-shi/"},"imageURL":"https://github.com/uuboyscy.png","key":"uuboyscy"}],"frontMatter":{"slug":"modern-data-engineering-milestones-key-technologies-that-shaped-the-industry","title":"Modern Data Engineering Milestones: Key Technologies That Shaped the Industry","authors":["uuboyscy"],"tags":["dbt","gcp","aws","airflow","prefect"]},"unlisted":false,"nextItem":{"title":"From Airflow to Prefect: Choosing the Right Orchestration Tool for Your Workflow","permalink":"/blog/from-airflow-to-prefect-choosing-the-right-orchestration-tool-for-your-workflow"}},"content":"```note\\nThis article is currently under migration and will be available soon.\\n```"},{"id":"from-airflow-to-prefect-choosing-the-right-orchestration-tool-for-your-workflow","metadata":{"permalink":"/blog/from-airflow-to-prefect-choosing-the-right-orchestration-tool-for-your-workflow","source":"@site/blog/2023-08-23-from-airflow-to-prefect-choosing-the-right-orchestration-tool-for-your-workflow/index.md","title":"From Airflow to Prefect: Choosing the Right Orchestration Tool for Your Workflow","description":"---","date":"2023-08-23T00:00:00.000Z","tags":[{"inline":false,"label":"GCP","permalink":"/blog/tags/gcp","description":"GCP tag description"},{"inline":false,"label":"Airflow","permalink":"/blog/tags/airflow","description":"Airflow tag description"},{"inline":false,"label":"Prefect","permalink":"/blog/tags/prefect","description":"Prefect tag description"}],"readingTime":3.355,"hasTruncateMarker":true,"authors":[{"name":"uuboyscy","title":"Data Engineer | Founder of uuboyscy.dev","url":"https://uuboyscy.dev","page":{"permalink":"/blog/authors/uuboyscy"},"socials":{"github":"https://github.com/uuboyscy","linkedin":"https://www.linkedin.com/in/chengyou-shi/"},"imageURL":"https://github.com/uuboyscy.png","key":"uuboyscy"}],"frontMatter":{"slug":"from-airflow-to-prefect-choosing-the-right-orchestration-tool-for-your-workflow","title":"From Airflow to Prefect: Choosing the Right Orchestration Tool for Your Workflow","authors":["uuboyscy"],"tags":["gcp","airflow","prefect"]},"unlisted":false,"prevItem":{"title":"Modern Data Engineering Milestones: Key Technologies That Shaped the Industry","permalink":"/blog/modern-data-engineering-milestones-key-technologies-that-shaped-the-industry"},"nextItem":{"title":"From MapReduce to Spark: The Evolution of Big Data Processing","permalink":"/blog/from-mapreduce-to-spark-the-evolution-of-big-data-processing"}},"content":"---\\n\\n## 1. Introduction\\nIn my role at a traditional company undergoing digital transformation, I faced a common challenge: managing multiple pipelines across Windows and Linux. These pipelines, essential for generating daily dashboard reports, were difficult to monitor, debug, and scale. \\n\\n\x3c!-- truncate --\x3e\\n\\nWhile Airflow is a popular choice for orchestration, it didn\u2019t fit well for our diverse team of engineers and analysts. Prefect, on the other hand, addressed our specific needs with its flexibility and user-friendly features.\\n\\n---\\n\\n## 2. The Problem\\nOur challenges included:\\n1. **Manual Execution**: Pipelines on Windows `ran manually`, while Linux relied on `crontab`.  \\n2. **Monitoring Issues**: No centralized way to monitor or trace errors.  \\n3. **Complexity for Analysts**: Analysts, skilled in SQL but less in Python, found it hard to contribute.  \\n4. **Scalability**: Adding new workers was cumbersome.\\n\\n---\\n\\n## 3. Why Airflow Wasn\u2019t Ideal\\n- **Steep Learning Curve**: Difficult for non-engineers to understand and use.  \\n- **Windows Support**: Running Airflow on Windows required workarounds.  \\n- **Scaling Issues**: Adding workers or scaling pipelines needed manual effort.\\n\\n---\\n\\n## 4. Why Prefect Was the Solution\\nPrefect solved our problems by offering:\\n- **User-Friendly Syntax**: Pythonic workflows easy for both engineers and analysts.  \\n- **Cross-Platform Support**: Worked seamlessly on both Windows and Linux.  \\n- **Real-Time Monitoring**: Built-in logging and error handling for quick debugging.  \\n- **Easy Scaling**: Agents simplified adding more workers as needed.\\n\\n---\\n\\n## 5. Steps to Transition\\n1. **Analyze Existing Pipelines**: Identify workflows and dependencies.  \\n2. **Set Up Prefect**: Install and configure Prefect for Windows and Linux.  \\n3. **Onboard Analysts**: Train analysts to contribute using SQL-friendly workflows.  \\n4. **Monitor and Debug**: Use Prefect\u2019s UI to track pipeline execution.  \\n5. **Scale When Needed**: Deploy agents for additional capacity.\\n\\n---\\n\\n## 6. Prefect vs. Airflow\\n| Feature              | Airflow                     | Prefect                     |\\n|----------------------|----------------------------|----------------------------|\\n| **Ease of Use**      | Complex for non-engineers  | Intuitive and Pythonic     |\\n| **Windows Support**  | Limited                   | Excellent                  |\\n| **Monitoring**       | Manual log tracing        | Built-in tools             |\\n| **Scalability**      | Requires expertise        | Simple agent setup         |\\n\\n---\\n\\n## 7. Airflow Code vs. Prefect Code: A Comparison\\nBelow is a side-by-side comparison of the same workflow implemented in `Airflow` and `Prefect`.\\n- Airflow code:\\n    ```Python\\n    from datetime import datetime, timedelta\\n\\n    from airflow import DAG\\n    from airflow.operators.bash import BashOperator\\n    from airflow.operators.python import PythonOperator\\n\\n\\n    def task1():\\n        print(\\"Running Task 1\\")\\n\\n    def task2():\\n        print(\\"Running Task 2\\")\\n\\n    # Default arguments for the DAG\\n    default_args = {\\n        \'owner\': \'airflow\',\\n        \'depends_on_past\': False,\\n        \'email\': [\'your_email@example.com\'],\\n        \'email_on_failure\': False,\\n        \'email_on_retry\': False,\\n        \'retries\': 1,\\n        \'retry_delay\': timedelta(minutes=5),\\n    }\\n\\n    # Define the DAG\\n    dag = DAG(\\n        \'d_02_example_dag_dependency\',\\n        default_args=default_args,\\n        description=\'An example DAG with Python operators\',\\n        schedule=\\"* * * * *\\",\\n        start_date=datetime(2023, 1, 1),\\n        catchup=False\\n    )\\n\\n    # Define the tasks\\n    task1_obj = PythonOperator(\\n        task_id=\'task1\',\\n        python_callable=task1,\\n        dag=dag,\\n    )\\n\\n    task2_obj = PythonOperator(\\n        task_id=\'task2\',\\n        python_callable=task2,\\n        dag=dag,\\n    )\\n\\n    task3_obj = BashOperator(\\n        task_id=\'task3\',\\n        bash_command=\'echo \\"Hello from Task 3!\\"\',\\n        dag=dag,\\n    )\\n\\n    # Task dependencies\\n    task1_obj >> task2_obj\\n    task1_obj >> task3_obj\\n    ```\\n- Prefect code:\\n    ```Python\\n    from prefect import flow, task\\n    from prefect.tasks.shell import ShellOperation\\n\\n\\n    @task\\n    def task1():\\n        print(\\"Running Task 1\\")\\n\\n\\n    @task\\n    def task2():\\n        print(\\"Running Task 2\\")\\n\\n\\n    @flow(name=\\"d_02_example_dag_dependency\\")\\n    def example_flow():\\n        # Define the tasks\\n        t1 = task1()\\n        t2 = task2()\\n        t3 = ShellOperation(commands=[\\"echo \'Hello from Task 3!\'\\"]).run()\\n\\n        # Set task dependencies\\n        t2.wait_for(t1)\\n        t3.wait_for(t1)\\n\\n\\n    if __name__ == \\"__main__\\":\\n        example_flow()\\n    ```\\n### Key Differences\\n\\n| Aspect                      | Airflow                                    | Prefect                                   |\\n|-----------------------------|-------------------------------------------|------------------------------------------|\\n| **Setup**                   | Requires defining a DAG explicitly.       | Uses Python functions and decorators.    |\\n| **Task Definition**         | Separate task objects (Python/Bash).      | Python `@task` decorator for simplicity. |\\n| **Dependency Management**   | Explicit with `>>` and `<<` operators.    | Handled with `.wait_for()` or function calls. |\\n| **Execution**               | Deployed to Airflow Scheduler.            | Runs as a Python script or with Prefect Cloud. |\\n| **Cross-Platform Support**  | Limited for Windows.                      | Cross-platform (Windows/Linux).          |\\n\\n\\n---\\n\\n## 8. Conclusion\\nPrefect\u2019s simplicity, flexibility, and user-friendly design make it ideal for diverse teams and hybrid environments. By transitioning to Prefect, we unified our workflows, empowered analysts, and improved scalability\u2014proving that the right tool can transform the way we work.\\n\\n---"},{"id":"from-mapreduce-to-spark-the-evolution-of-big-data-processing","metadata":{"permalink":"/blog/from-mapreduce-to-spark-the-evolution-of-big-data-processing","source":"@site/blog/2022-08-23-from-mapreduce-to-spark-the-evolution-of-big-data-processing/index.md","title":"From MapReduce to Spark: The Evolution of Big Data Processing","description":"1. Introduction: Big Data Challenges","date":"2022-08-23T00:00:00.000Z","tags":[{"inline":false,"label":"Hadoop","permalink":"/blog/tags/hadoop","description":"Hadoop tag description"},{"inline":false,"label":"Spark","permalink":"/blog/tags/spark","description":"Spark tag description"}],"readingTime":3.365,"hasTruncateMarker":true,"authors":[{"name":"uuboyscy","title":"Data Engineer | Founder of uuboyscy.dev","url":"https://uuboyscy.dev","page":{"permalink":"/blog/authors/uuboyscy"},"socials":{"github":"https://github.com/uuboyscy","linkedin":"https://www.linkedin.com/in/chengyou-shi/"},"imageURL":"https://github.com/uuboyscy.png","key":"uuboyscy"}],"frontMatter":{"slug":"from-mapreduce-to-spark-the-evolution-of-big-data-processing","title":"From MapReduce to Spark: The Evolution of Big Data Processing","authors":["uuboyscy"],"tags":["hadoop","spark"]},"unlisted":false,"prevItem":{"title":"From Airflow to Prefect: Choosing the Right Orchestration Tool for Your Workflow","permalink":"/blog/from-airflow-to-prefect-choosing-the-right-orchestration-tool-for-your-workflow"},"nextItem":{"title":"Welcome","permalink":"/blog/welcome"}},"content":"## 1. Introduction: Big Data Challenges\\n\\nBig data means working with very large amounts of information. In one of my jobs, I had to handle 500TB of data and run more than 10,000 SQL queries every day. The old system we used was slow and had many problems, like some tasks taking over 24 hours to finish. In this blog, I will share how I solved these problems by using Spark and making the system faster and better.\\n\\n\x3c!-- truncate --\x3e\\n\\n---\\n\\n## 2. What is Hadoop Hive and MapReduce?\\n\\nHadoop Hive is a tool that helps process big data using SQL queries. It works with a system called MapReduce, which was great when it was created but has many limits now:\\n\\n- **Processes One Query at a Time**: It could not run many queries at once.\\n- **Very Slow**: Large or complicated tasks took a long time.\\n- **Not Efficient**: It used too many resources for simple tasks.\\n\\nIn my work, these issues caused delays, and some tasks took more than one day to finish.\\n\\n---\\n\\n## 3. Problems with the Old System\\n\\nThe old system was not good enough for our needs:\\n\\n- **One Query per Task**: Each SQL query was treated as a separate job, wasting resources.\\n- **Huge Data Size**: Processing 100TB of data made it even harder.\\n- **Long Task Times**: Some pipelines (multiple tasks) ran for over 24 hours.\\n\\nThese problems showed we needed a better solution.\\n\\n---\\n\\n## 4. Switching to Spark\\n\\nSpark is much faster and better than MapReduce for big data processing. It helped solve our problems because:\\n\\n- **Faster Processing**: Spark processes data in memory, so tasks are much quicker.\\n- **Runs Tasks at the Same Time**: Spark can handle many tasks at once, saving time.\\n- **Easy to Use**: Spark has a SQL tool that works like Hive, making the switch simple.\\n\\nAfter replacing MapReduce with Spark, the system became much faster and could handle more work.\\n\\n---\\n\\n## 5. How I Improved the System\\n\\nSwitching to Spark was just the first step. \\n![mapreduce-to-spark](./mapreduce-to-spark.png)\\n\\nAfter that, I focused on restructuring the system to make it even more efficient.Here\u2019s what I did:\\n\\n1. **Refactored the Program Architecture**: I modified the structure so that a single task could execute multiple SQL queries at the same time. This reduced the overhead of creating separate tasks for each query, saving both time and resources.\\n![single-product-process](./single-product-process.png)\\n\\n2. **Developed an API for SQL Execution**: Since other departments, such as analysts, wanted to use Spark to run SQL but didn\u2019t have programming skills, I created an API called **jobQueue API**. This API allowed them to execute their SQL queries easily:\\n   - They only needed to know how to make an API request.\\n   - They could specify the SQL queries they wanted to execute in the request.\\n   - The API handled the processing on Spark, abstracting away all the complexity.\\n  \\n    ![api-execution-monitor](./api-execution-monitor.png)\\n\\n1. **Run Tasks Together**: By leveraging Spark\u2019s ability to execute tasks in parallel, I optimized query execution times further, ensuring the system could handle a large number of queries more efficiently.\\n![multiple-product-process](./multiple-product-process.png)\\n\\n2. **Monitor Progress**: Spark\u2019s user interface was helpful for tracking task progress and debugging. I used this to ensure everything was running smoothly and to quickly fix any issues.\\n\\nWith these improvements, the system could run 10,000 SQL queries seamlessly within 3 hours. The addition of the **jobQueue API** also empowered other teams to use Spark without needing deep technical knowledge, making the system more accessible and collaborative.\\n![time-spend-chart](./time-spend-chart.png)\\n\\n---\\n\\n## 6. What I Learned\\n\\nHere are some important lessons from this project:\\n\\n- **Small Changes Can Help**: Switching to Spark and combining queries made a huge difference in performance and efficiency.\\n- **Use the Right Tool**: Spark\'s features like in-memory processing and parallel execution were perfect for our workload.\\n- **Be Efficient**: Designing the system to execute multiple SQL queries in a single task saved both time and resources.\\n\\nThese lessons highlight the importance of choosing the right approach and tools when working with big data."},{"id":"welcome","metadata":{"permalink":"/blog/welcome","source":"@site/blog/1992-12-11-welcom.md","title":"Welcome","description":"Hey there! Welcome to my very first blog post! \ud83c\udf89","date":"1992-12-11T00:00:00.000Z","tags":[{"inline":false,"label":"Hello","permalink":"/blog/tags/hello","description":"Hello tag description"},{"inline":false,"label":"Docusaurus","permalink":"/blog/tags/docusaurus","description":"Docusaurus tag description"}],"readingTime":1.09,"hasTruncateMarker":true,"authors":[{"name":"uuboyscy","title":"Data Engineer | Founder of uuboyscy.dev","url":"https://uuboyscy.dev","page":{"permalink":"/blog/authors/uuboyscy"},"socials":{"github":"https://github.com/uuboyscy","linkedin":"https://www.linkedin.com/in/chengyou-shi/"},"imageURL":"https://github.com/uuboyscy.png","key":"uuboyscy"}],"frontMatter":{"slug":"welcome","title":"Welcome","authors":["uuboyscy"],"tags":["hello","docusaurus"]},"unlisted":false,"prevItem":{"title":"From MapReduce to Spark: The Evolution of Big Data Processing","permalink":"/blog/from-mapreduce-to-spark-the-evolution-of-big-data-processing"}},"content":"Hey there! Welcome to my very first blog post! \ud83c\udf89\\n\\nI\'m super excited to start sharing my thoughts and experiences with you all through this platform. As someone who\'s passionate about technology and programming, I figured it\'s about time I carved out my own little corner of the internet.\\n\\n\x3c!-- truncate --\x3e\\n\\nYou know what\'s funny? I spent way too much time deciding what to write for my first post. Should it be technical? Personal? A mix of both? In the end, I decided to just be myself and write whatever comes to mind.\\n\\nI\'ve been working in tech for a while now, and there\'s always something new to learn. Whether it\'s a new programming language, a cool framework, or an interesting design pattern, the learning never stops. And that\'s exactly what I love about this field!\\n\\nHere\'s a random list of things I\'m currently excited about:\\n- Building this blog with Docusaurus\\n- Exploring new technologies\\n- Sharing knowledge with the community\\n- Learning from others\' experiences\\n\\nI plan to write about:\\n1. Technical tutorials\\n2. Programming tips and tricks\\n3. Personal experiences in tech\\n4. Random thoughts about software development\\n\\nStay tuned for more posts! Feel free to reach out if you want to discuss anything tech-related (or just want to say hi)!\\n\\nHappy coding! \ud83d\udcbb"}]}}')}}]);